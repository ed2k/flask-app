{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Elephant\n",
      "Embedding size: 512\n",
      "Embedding: [0.008344483561813831, 0.0004808558733202517, 0.06595248728990555, ...]\n",
      "\n",
      "Message: elephant is animal\n",
      "Embedding size: 512\n",
      "Embedding: [-0.02572762779891491, -0.00338289188221097, 0.06081477552652359, ...]\n",
      "\n",
      "Message: elephant is animalThere is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
      "Embedding size: 512\n",
      "Embedding: [-0.02780488319694996, -0.028051231056451797, -0.013840027153491974, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import numpy as np\n",
    "\n",
    "#@title Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"elephant is animal\"\n",
    "paragraph = (\n",
    "    \"elephant is animal\"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "message_embeddings = embed(messages)\n",
    "\n",
    "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "  print(\"Message: {}\".format(messages[i]))\n",
    "  print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "  message_embedding_snippet = \", \".join(\n",
    "      (str(x) for x in message_embedding[:3]))\n",
    "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "Collecting horapy\n",
      "\u001b[33m  WARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "  Downloading horapy-0.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "\u001b[K     |████████████████████████████████| 427 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /home/a/anaconda3/lib/python3.7/site-packages (from horapy) (1.19.4)\n",
      "Installing collected packages: horapy\n",
      "Successfully installed horapy-0.0.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951 in Hora ANNIndex <HNSWIndexUsize> (dimension: 512, dtype: usize, max_item: 1000000, n_neigh: 32, n_neigh0: 64, ef_build: 20, ef_search: 500, has_deletion: False) \n",
      "has neighbors: [951, 576, 799, 987, 384, 495, 434, 348, 445, 632]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from horapy import HNSWIndex\n",
    "\n",
    "dimension = 512\n",
    "n = 1000\n",
    "\n",
    "# init index instance\n",
    "index = HNSWIndex(dimension, \"usize\")\n",
    "\n",
    "samples = np.float32(np.random.rand(n, dimension))\n",
    "for i in range(0, len(samples)):\n",
    "    # add node\n",
    "    index.add(np.float32(samples[i]), i)\n",
    "\n",
    "index.build(\"euclidean\")  # build index\n",
    "\n",
    "target = np.random.randint(0, n)\n",
    "# 410 in Hora ANNIndex <HNSWIndexUsize> (dimension: 50, dtype: usize, max_item: 1000000, n_neigh: 32, n_neigh0: 64, ef_build: 20, ef_search: 500, has_deletion: False)\n",
    "# has neighbors: [410, 736, 65, 36, 631, 83, 111, 254, 990, 161]\n",
    "print(\"{} in {} \\nhas neighbors: {}\".format(\n",
    "    target, index, index.search(samples[target], 10)))  # search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10001\n",
      "20001\n",
      "30001\n",
      "40001\n",
      "50001\n",
      "build index\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "index = HNSWIndex(dimension, \"usize\")\n",
    "knowledge_dict = dict()\n",
    "NEWLINE = '\\n'\n",
    "def convert_one_file(filename, target_fd):\n",
    "    \"\"\"prefix list of keywords\n",
    "    \"\"\"\n",
    "    buffer = list()\n",
    "    progress = 0\n",
    "    for line in open(filename):\n",
    "        if line.strip() == '':\n",
    "            if buffer:\n",
    "                progress += 1\n",
    "                message = ''.join(buffer)\n",
    "                # print(buffer)\n",
    "                message_embeddings = embed([message])\n",
    "                message_embedding = np.array(message_embeddings).tolist()[0]\n",
    "                # print(message_embedding)\n",
    "                message_embedding_snippet = \", \".join((str(x) for x in message_embedding))\n",
    "#                 v = f\"embedding: {message_embedding_snippet}\"\n",
    "#                 target_fd.write(v + NEWLINE)\n",
    "#                 target_fd.write(message)\n",
    "#                 target_fd.write(NEWLINE)\n",
    "                knowledge_dict[progress] = message\n",
    "                index.add(np.float32(message_embedding), progress)\n",
    "                buffer =  list()\n",
    "                if progress % 10000 == 1:\n",
    "                    print(progress)\n",
    "        else:\n",
    "            buffer.append(line)\n",
    "\n",
    "convert_one_file('/home/a/knowledge/test.txt', open('testv.txt', 'w'))\n",
    "print('build index')\n",
    "index.build(\"euclidean\")  # build index\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ideas learn-anything !\n",
      "![](https://i.imgur.com/CXLG4IY.jpg)\n",
      "\n",
      "computer-science our\n",
      "[Our Thoughts on P=NP (2020)](https://rjlipton.wordpress.com/2020/01/12/our-thoughts-on-pnp/)\n",
      "\n",
      "future ability\n",
      "Ability to control computers with thoughts at high bandwidth.\n",
      "\n",
      "ideas learn-anything build\n",
      "[Build your own X](https://github.com/danistefanovic/build-your-own-x)\n",
      "\n",
      "physics string-theory the biggest\n",
      "[The Biggest Ideas in the Universe (2020)](https://www.youtube.com/playlist?list=PLrxfgDEc2NxZJcWcrxH3jyjUUrJlnoyzX)\n",
      "\n",
      "future progress\n",
      "[Progress](https://patrickcollison.com/progress)\n",
      "\n",
      "ideas learn-anything are\n",
      "[Are Ideas Getting Harder to Find? (2020)](https://web.stanford.edu/~chadj/IdeaPF.pdf)\n",
      "\n",
      "math mathpages\n",
      "[MathPages](https://www.mathpages.com/home/index.htm)\n",
      "\n",
      "mindfulness meditation treating\n",
      "Treating thoughts and thinking in this way is incredibly freeing. Not getting attached to what you think. And not getting attached to anything is incredibly empowering feeling. But it is something that you have to practice often.\n",
      "\n",
      "focusing the power\n",
      "[The Power of Shower Thoughts: Trusting Your Mind to Work in the Background (2019)](https://alexanderell.is/posts/trust-in-your-unconscious/) ([HN](https://news.ycombinator.com/item?id=21557902))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.float32(np.random.rand(n, dimension))\n",
    "query = 'deepest thoughts'\n",
    "message_embeddings = embed([query])\n",
    "message_embedding = np.array(message_embeddings).tolist()[0]\n",
    "\n",
    "results = index.search(message_embedding, 10)\n",
    "for i in results:\n",
    "    print(knowledge_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
